{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copia di AML_Project.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "UXG6e2PquoqN",
        "unyTDi0vbuRC",
        "3iZSnFTLNDim",
        "YgMUIGbwwtHO",
        "H4eBCcS-wmKI",
        "oF8XFzrUwwXz"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/g185/AMLrepository/blob/main/Copia_di_AML_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mKzPUY-32Av"
      },
      "source": [
        "**To Do**\r\n",
        "\r\n",
        "*   Add Params in order to start make the two datasets parametric\r\n",
        "(Project Parameters + Data extraction & preprocessing)\r\n",
        "*   Loading Function\r\n",
        "(Training + Loading)\r\n",
        "*   Example\r\n",
        "(Example)\r\n",
        "*   Add information about size of dataset in parameters\r\n",
        "(Projexct Parameters)\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXG6e2PquoqN"
      },
      "source": [
        "#***Download and unzip Dataset***\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhlPBhD98p94"
      },
      "source": [
        "*   Create folders\n",
        "*   Download Recipe 5K + Annotations in drive\n",
        "*   Unzip files in folder\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUrWflPRudbx"
      },
      "source": [
        "#Create folders for models and datasets\n",
        "!mkdir \"/content/drive/My Drive/RecipeNet\" \n",
        "!mkdir \"/content/drive/My Drive/RecipeNet/datasets\" \n",
        "!mkdir \"/content/drive/My Drive/RecipeNet/datasets/download\" \n",
        "!mkdir \"/content/drive/My Drive/RecipeNet/datasets/extracted\" \n",
        "!mkdir \"/content/drive/My Drive/RecipeNet/datasets/preprocessed\" \n",
        "!mkdir \"/content/drive/My Drive/RecipeNet/model\" \n",
        "#Scarica dataset\n",
        "#Trascina i 3 zip\n",
        "\n",
        "#Unzip\n",
        "\"\"\"\n",
        "!unzip \"/content/drive/My Drive/RecipeNet/datasets/download/new_right_train.zip\" -d \"/content/drive/My Drive/RecipeNet_g/datasets/extracted\"\n",
        "!unzip \"/content/drive/My Drive/RecipeNet/datasets/download/archive.zip\" -d \"/content/drive/My Drive/RecipeNet/datasets/extracted\"\n",
        "!unzip \"/content/drive/My Drive/RecipeNet/datasets/download/Ingredients101.zip\" -d \"/content/drive/My Drive/RecipeNet/datasets/extracted\"\n",
        "\n",
        "\"\"\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YT-TwMVuBLEv"
      },
      "source": [
        "#***Imports and Drive Mount***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOJPDp-bU6rr",
        "outputId": "d5878324-230d-48e3-df3d-82954ae42cf0"
      },
      "source": [
        "#Imports\n",
        "import sys\n",
        "import time\n",
        "import os\n",
        "import json\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "from tqdm.notebook import tqdm, trange\n",
        "import sys\n",
        "import cv2\n",
        "import torch\n",
        "import torch.utils.data as data\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from glob import glob\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.models as models\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "#Cuda\n",
        "device = torch.device(\"cuda\")\n",
        "torch.manual_seed(42) # try and make the results more reproducible\n",
        "print(torch.cuda.get_device_name(0))\n",
        "\n",
        "#Drive Mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tesla P100-PCIE-16GB\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_AXoIoMB0om"
      },
      "source": [
        "#***Project Parameters***\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYIvhj8aB4NV"
      },
      "source": [
        "params = {}\n",
        "\n",
        "#DATASET KEYWORD\n",
        "params[\"dataset\"] = \"Recipe5k\"\n",
        "\n",
        "#NEURAL NETWORK PARAMS\n",
        "params[\"freezed_layers\"] = 8\n",
        "\n",
        "#GENERAL TRAINING PARAMS\n",
        "params[\"epochs\"] = 20\n",
        "params[\"batch_size\"] = 64\n",
        "params[\"img_size\"] = (384,384)\n",
        "\n",
        "#FAST TRAINING SETTINGS\n",
        "#1M: TR:67000 VAL:5400 TE:27000\n",
        "#5k: TR: VAL: TE:\n",
        "params[\"fast_training\"] = True\n",
        "params[\"train_df_size\"] = 120\n",
        "params[\"val_df_size\"] = 120\n",
        "params[\"test_df_size\"] = 120\n",
        "\n",
        "\n",
        "#DATASET SETTINGS\n",
        "params[\"root\"] = \"/content/drive/My Drive/RecipeNet/\"\n",
        "if params[\"dataset\"] == \"Recipe5k\":\n",
        "    params[\"images_dir\"] = params[\"root\"] + \"datasets/extracted/images/\" \n",
        "    params[\"ingredients_per_class\"] = params[\"root\"] + \"datasets/extracted/Ingredients101/Annotations/ingredients_simplified.txt\" \n",
        "    params[\"classes\"] = params[\"root\"] + \"datasets/extracted/Ingredients101/Annotations/classes.txt\" \n",
        "    params[\"baseIngredients_dir\"] = params[\"root\"] + \"datasets/extracted/Ingredients101/ingredients_simplification/baseIngredients.txt\" \n",
        "    params[\"annotations_path\"] = params[\"root\"] + \"datasets/extracted/Ingredients101/Annotations/\"\n",
        "elif params[\"dataset\"] == \"Recipe1M\":\n",
        "    params[\"images_dir\"] = params[\"root\"] + \"datasets/extracted/new_right_train/\" \n",
        "    params[\"annotations_path\"] = params[\"root\"] + \"datasets/extracted/Annotations_1M/\"\n",
        "    params[\"ingredients_per_class\"] = params[\"root\"] + \"datasets/extracted/Annotations_1M/ingr.txt\" \n",
        "    params[\"classes\"] = params[\"root\"] + \"datasets/extracted/Annotations_1M/newclasses.txt\" \n",
        "    params[\"baseIngredients_dir\"] = params[\"root\"] + \"datasets/extracted/Annotations_1M/baseIngredients.txt\" \n",
        "    params[\"annotations_path\"] = params[\"root\"] + \"datasets/extracted/Annotations_1M/\"\n",
        "else:\n",
        "    print(\"Invalid Dataset Keyword, try with Recipe1M or Recipe5k\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzkRmamqNDt_"
      },
      "source": [
        "#***Data extraction and preprocessing***\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5tPzq9WjV77"
      },
      "source": [
        "Dataset Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBw5OIEM8Gw5"
      },
      "source": [
        "#Ingredients x class\r\n",
        "f = open(params[\"ingredients_per_class\"], \"r\")\r\n",
        "ingredients = f.read().split('\\n')\r\n",
        "f.close()\r\n",
        "\r\n",
        "#Classes\r\n",
        "f = open(params[\"classes\"], \"r\")\r\n",
        "classes = f.read().split('\\n')\r\n",
        "f.close()\r\n",
        "\r\n",
        "#Ingredients\r\n",
        "f = open(params[\"baseIngredients_dir\"], \"r\")\r\n",
        "base_ing = f.read().split('\\n')\r\n",
        "base_ing = base_ing[0].split(\",\")\r\n",
        "f.close()\r\n",
        "\r\n",
        "#train images\r\n",
        "f = open(params[\"root\"] + \"datasets/extracted/Ingredients101/Annotations/train_images.txt\", \"r\")\r\n",
        "train_images = f.read().split('\\n')\r\n",
        "f.close()\r\n",
        "f = open(params[\"root\"] + \"datasets/extracted/Ingredients101/Annotations/train_labels.txt\", \"r\")\r\n",
        "train_labels = f.read().split('\\n')\r\n",
        "f.close()\r\n",
        "\r\n",
        "#validation images\r\n",
        "f = open(params[\"root\"] + \"datasets/extracted/Ingredients101/Annotations/val_images.txt\", \"r\")\r\n",
        "val_images = f.read().split('\\n')\r\n",
        "f.close()\r\n",
        "f = open(params[\"root\"] + \"datasets/extracted/Ingredients101/Annotations/val_labels.txt\", \"r\")\r\n",
        "val_labels = f.read().split('\\n')\r\n",
        "f.close()\r\n",
        "\r\n",
        "#test images\r\n",
        "f = open(params[\"root\"] + \"datasets/extracted/Ingredients101/Annotations/test_images.txt\", \"r\")\r\n",
        "test_images = f.read().split('\\n')\r\n",
        "f.close()\r\n",
        "f = open(params[\"root\"] + \"datasets/extracted/Ingredients101/Annotations/test_labels.txt\", \"r\")\r\n",
        "test_labels = f.read().split('\\n')\r\n",
        "f.close()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtUqblzXja33"
      },
      "source": [
        "#Ingredients x class\n",
        "f = open(params[\"ingredients_per_class\"], \"r\")\n",
        "ingredients = f.read().split('\\n')\n",
        "f.close()\n",
        "\n",
        "#Classes\n",
        "f = open(params[\"classes\"], \"r\")\n",
        "classes = f.read().split('\\n')\n",
        "f.close()\n",
        "\n",
        "#Ingredients\n",
        "f = open(params[\"baseIngredients_dir\"], \"r\")\n",
        "base_ing = f.read().split('\\n')\n",
        "base_ing = base_ing[0].split(\",\")\n",
        "f.close()\n",
        "\n",
        "#train images\n",
        "f = open(params[\"root\"] + \"datasets/extracted/Annotations_1M/train_imgs.txt\", \"r\")\n",
        "train_images = f.read().split('\\n')\n",
        "f.close()\n",
        "f = open(params[\"root\"] + \"datasets/extracted/Annotations_1M/train_lab.txt\", \"r\")\n",
        "train_labels = f.read().split('\\n')\n",
        "f.close()\n",
        "\n",
        "#validation images\n",
        "f = open(params[\"root\"] + \"datasets/extracted/Annotations_1M/val_imgs.txt\", \"r\")\n",
        "val_images = f.read().split('\\n')\n",
        "f.close()\n",
        "f = open(params[\"root\"] + \"datasets/extracted/Annotations_1M/val_lab.txt\", \"r\")\n",
        "val_labels = f.read().split('\\n')\n",
        "f.close()\n",
        "\n",
        "#test images\n",
        "f = open(params[\"root\"] + \"datasets/extracted/Annotations_1M/test_imgs.txt\", \"r\")\n",
        "test_images = f.read().split('\\n')\n",
        "f.close()\n",
        "f = open(params[\"root\"] + \"datasets/extracted/Annotations_1M/test_lab.txt\", \"r\")\n",
        "test_labels = f.read().split('\\n')\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URPn4rnyjjoj"
      },
      "source": [
        "Dataframes Creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOoX41Sdjm3A"
      },
      "source": [
        "#list of string in list of list of tokens\n",
        "new_ingredients = [arr.split(\",\") for arr in ingredients]\n",
        "\n",
        "#binary encode ingredients\n",
        "mlb = MultiLabelBinarizer()\n",
        "df = pd.DataFrame(mlb.fit_transform(new_ingredients),columns=mlb.classes_) \n",
        "df[\"target\"] = classes\n",
        "food_dict = df\n",
        "train_images = [params[\"images_dir\"] + s + \".jpg\" for s in train_images]\n",
        "all_img_df = pd.DataFrame({'path': train_images, 'class_id': train_labels})\n",
        "val_images = [params[\"images_dir\"] + s + \".jpg\" for s in val_images]\n",
        "val_img_df = pd.DataFrame({'path': val_images, 'class_id': val_labels})\n",
        "test_images = [params[\"images_dir\"] + s + \".jpg\" for s in test_images]\n",
        "test_img_df = pd.DataFrame({'path': test_images, 'class_id': test_labels})\n",
        "all_img_df = all_img_df[:-1]\n",
        "val_img_df = val_img_df[:-1]\n",
        "test_img_df = test_img_df[:-1]\n",
        "\n",
        "\n",
        "\n",
        "all_img_df['class_name'] = all_img_df['path'].map(lambda x: os.path.split(os.path.dirname(x))[-1])\n",
        "val_img_df['class_name'] = val_img_df['path'].map(lambda x: os.path.split(os.path.dirname(x))[-1])\n",
        "test_img_df['class_name'] = test_img_df['path'].map(lambda x: os.path.split(os.path.dirname(x))[-1])\n",
        "\n",
        "#food_dict = food_dict.drop('', 1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "468oZbcTjzjI"
      },
      "source": [
        "Train Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "su9RKGPwj3M7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3610ad5-7163-4462-9181-1448ec8a5d32"
      },
      "source": [
        "#Dataframe for train images\n",
        "new_data = []\n",
        "for index, row in all_img_df.iterrows():\n",
        "    #get binary encoding ingredients from lookup\n",
        "    food = row[\"class_name\"]\n",
        "    path = row[\"path\"]\n",
        "    class_id = row[\"class_id\"]\n",
        "    binary_encod = food_dict.loc[food_dict[\"target\"] == food]\n",
        "    binary_encod[\"path\"] = path\n",
        "    binary_encod[\"class_id\"] = class_id\n",
        "    #print(binary_encod[\"class_id\"])\n",
        "    #print((list(binary_encod.columns.values)))\n",
        "    #print(len(np.array(binary_encod)[0]))\n",
        "    new_data.append(np.array(binary_encod)[0])\n",
        "\n",
        "\n",
        "\n",
        "col_names = list(binary_encod.columns.values)\n",
        "print(len(col_names))\n",
        "train_df = pd.DataFrame(new_data, columns = col_names)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "231\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTtejOUlj4l2"
      },
      "source": [
        "Validation Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FE5pj15Ij75W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8121fdbd-3b1c-4ed8-b727-eb1b98773b53"
      },
      "source": [
        "val_data = []\n",
        "for index, row in val_img_df.iterrows():\n",
        "    #get binary encoding ingredients from lookup\n",
        "    food = row[\"class_name\"]\n",
        "    path = row[\"path\"]\n",
        "    class_id = row[\"class_id\"]\n",
        "    binary_encod = food_dict.loc[food_dict[\"target\"] == food]\n",
        "    binary_encod[\"path\"] = path\n",
        "    binary_encod[\"class_id\"] = int(class_id)\n",
        "    val_data.append(np.array(binary_encod)[0])\n",
        "val_df = pd.DataFrame(val_data, columns = col_names)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHK2_X4fj-l_"
      },
      "source": [
        "Test Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzdWTpbZeyap",
        "outputId": "79de3942-abc8-4c0f-ef12-daac686211b7"
      },
      "source": [
        "test_data = []\n",
        "for index, row in test_img_df.iterrows():\n",
        "    #get binary encoding ingredients from lookup\n",
        "    food = row[\"class_name\"]\n",
        "    path = row[\"path\"]\n",
        "    class_id = row[\"class_id\"]\n",
        "    binary_encod = food_dict.loc[food_dict[\"target\"] == food]\n",
        "    binary_encod[\"path\"] = path\n",
        "    binary_encod[\"class_id\"] = int(class_id)\n",
        "    test_data.append(np.array(binary_encod)[0])\n",
        "\n",
        "\n",
        "test_df = pd.DataFrame(test_data, columns = col_names)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unyTDi0vbuRC"
      },
      "source": [
        "#***DataGenerator***\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CtfDNypczIt"
      },
      "source": [
        "class DataWrapper(data.Dataset):\n",
        "    ''' Data wrapper for pytorch's data loader function '''\n",
        "    def __init__(self, image_df, resize):\n",
        "        self.dataset = image_df\n",
        "        self.resize = resize\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        c_row = self.dataset.iloc[index]\n",
        "        target_arr = []\n",
        "        for item in c_row[targets].values:\n",
        "            target_arr.append(item)\n",
        "        #print(target_arr)\n",
        "        image_path, target = c_row['path'], torch.from_numpy(np.array(target_arr)).float()  #image and target\n",
        "        #read as rgb image, resize and convert to range 0 to 1\n",
        "        image = cv2.imread(image_path, 1)\n",
        "        #print(image_path, image)\n",
        "        if self.resize:\n",
        "            image = cv2.resize(image, params[\"img_size\"])/255.0 \n",
        "        else:\n",
        "            image = image/255.0\n",
        "        image = (torch.from_numpy(image.transpose(2,0,1))).float() #NxCxHxW\n",
        "        return image, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.dataset.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iZSnFTLNDim"
      },
      "source": [
        "#***Trasfer Learning Model:ResNet***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bWUNYL4wdKR"
      },
      "source": [
        "col_names = list(train_df.columns.values)\n",
        "targets = col_names[:-3]\n",
        "\n",
        "model = models.resnet50(pretrained=True)\n",
        "\n",
        "output = len(targets)\n",
        "model.fc = nn.Linear(model.fc.in_features, output)\n",
        "\n",
        "layer = 0\n",
        "for name, child in model.named_children():\n",
        "    layer += 1\n",
        "    if layer < params[\"freezed_layers\"]:\n",
        "        for name2, param in child.named_parameters():\n",
        "            param.requires_grad = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgMUIGbwwtHO"
      },
      "source": [
        "#***Training***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tejkwYHPhx5o"
      },
      "source": [
        "Define Hamming Score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1l7jgF4h83W"
      },
      "source": [
        "def hamming_score(y_true, y_pred, normalize=True, sample_weight=None):\n",
        "    '''\n",
        "    Compute the Hamming score (a.k.a. label-based accuracy) for the multi-label case\n",
        "    https://stackoverflow.com/q/32239577/395857\n",
        "    '''\n",
        "    acc_list = []\n",
        "    for i in range(y_true.shape[0]):\n",
        "        set_true = set( np.where(y_true[i])[0] )\n",
        "        set_pred = set( np.where(y_pred[i])[0] )\n",
        "        tmp_a = None\n",
        "        if len(set_true) == 0 and len(set_pred) == 0:\n",
        "            tmp_a = 1\n",
        "        else:\n",
        "            tmp_a = len(set_true.intersection(set_pred))/\\\n",
        "                    float( len(set_true.union(set_pred)) )\n",
        "        acc_list.append(tmp_a)\n",
        "    return np.mean(acc_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5P5t3kOiWyN"
      },
      "source": [
        "Training Settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAGn6B-hiZlR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "629cc4d8-6dbc-4eae-a4e7-05803d88a2dd"
      },
      "source": [
        "if params[\"fast_training\"] == True:\n",
        "    train_df = train_df[:params[\"train_df_size\"]]\n",
        "    val_df = val_df[:params[\"val_df_size\"]]\n",
        "    test_df = test_df[:params[\"test_df_size\"]]\n",
        "            \n",
        "print(len(val_df))\n",
        "model = model.to(device)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "120\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4BOAUZSif36"
      },
      "source": [
        "Dataset Loaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fN34WEC3iiLB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "outputId": "eed08cec-034c-4ca8-a5b6-1bcc26b975e6"
      },
      "source": [
        "train_dataset = DataWrapper(train_df, True)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset,shuffle=True, batch_size=params[\"batch_size\"], pin_memory=False)\n",
        "\n",
        "val_dataset = DataWrapper(val_df, True)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset,shuffle=True, batch_size=params[\"batch_size\"], pin_memory=False)\n",
        "\n",
        "test_dataset = DataWrapper(test_df, True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset,shuffle=True, batch_size=params[\"batch_size\"], pin_memory=False)\n",
        "\n",
        "#Definisco i plots\n",
        "train_results = defaultdict(list)\n",
        "train_iter, test_iter, best_acc = 0,0,0\n",
        "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize = (10, 10))\n",
        "ax1.set_title('Train Loss')\n",
        "ax2.set_title('Train Accuracy')\n",
        "ax3.set_title('Test Loss')\n",
        "ax4.set_title('Test Accuracy')\n",
        "\n",
        "#dizionario f1 scores\n",
        "f1_scores = defaultdict(list)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 4 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCjVOOL4i48a"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THX6oDqxqblX"
      },
      "source": [
        "\n",
        "for i in trange(params[\"epochs\"], desc='Epochs'):\n",
        "    print(\"Epoch \",i)\n",
        "    ## Train Phase\n",
        "    #Model switches to train phase\n",
        "    model.train() \n",
        "    \n",
        "    #azzera ris\n",
        "    all_outputs = []\n",
        "    all_targets = []\n",
        "\n",
        "    # Running through all mini batches in the dataset\n",
        "    count, loss_val, correct, total = train_iter, 0, 0, 0\n",
        "    for img_data, target in tqdm(train_loader, desc='Training'):  \n",
        "\n",
        "        #load batch  \n",
        "        img_data, target = img_data.to(device), target.to(device)\n",
        "        #FWD prop\n",
        "        output = model(img_data) \n",
        "\n",
        "        #Cross entropy loss\n",
        "        loss = criterion(output, target) \n",
        "\n",
        "        #current loss \n",
        "        c_loss = loss.data.item()\n",
        "\n",
        "        #plot current loss\n",
        "        ax1.plot(count, c_loss, 'r.')\n",
        "\n",
        "        #accumulate loss for training phase\n",
        "        loss_val += c_loss\n",
        "\n",
        "        optimizer.zero_grad() #Zero out any cached gradients\n",
        "        loss.backward() #Backward pass\n",
        "        optimizer.step() #Update the weights\n",
        "\n",
        "        #number of outputs, batch * labels\n",
        "        total_batch = (target.size(0) * target.size(1))\n",
        "        total += total_batch\n",
        "\n",
        "        #output and targets\n",
        "        output_data = torch.sigmoid(output)>=0.5\n",
        "        target_data = (target==1.0)\n",
        "        \n",
        "        #carica i risultati e i target\n",
        "        for arr1,arr2 in zip(output_data, target_data):\n",
        "            all_outputs.append(list(arr1.cpu().numpy()))\n",
        "            all_targets.append(list(arr2.cpu().numpy()))\n",
        "\n",
        "        #accuracy\n",
        "        c_acc = torch.sum((output_data == target_data.to(device)).to(torch.float)).item()\n",
        "        \n",
        "        #plot accuracy\n",
        "        ax2.plot(count, c_acc/total_batch, 'r.')\n",
        "\n",
        "        #accumula accuracy\n",
        "        correct += c_acc\n",
        "\n",
        "        #accumula numero di steps per epoch\n",
        "        count +=1\n",
        "        \n",
        "    #prendi i risultati dell'ultimo training\n",
        "    all_outputs = np.array(all_outputs)\n",
        "    all_targets = np.array(all_targets)\n",
        "\n",
        "    #Training Metrics\n",
        "    #______________________________________________________________________________________________________\n",
        "    f1score_samples = f1_score(y_true=all_targets, y_pred=all_outputs, average='samples')\n",
        "    f1score_macro = f1_score(y_true=all_targets, y_pred=all_outputs, average='macro')\n",
        "    f1score_weighted = f1_score(y_true=all_targets, y_pred=all_outputs, average='weighted')\n",
        "    recall = recall_score(y_true=all_targets, y_pred=all_outputs, average='samples')\n",
        "    prec = precision_score(y_true=all_targets, y_pred=all_outputs, average='samples')\n",
        "    hamming = hamming_score(y_true=all_targets, y_pred=all_outputs)\n",
        "    \n",
        "    f1_scores[\"samples_train\"].append(f1score_samples)\n",
        "    f1_scores[\"macro_train\"].append(f1score_macro)\n",
        "    f1_scores[\"weighted_train\"].append(f1score_weighted)\n",
        "    f1_scores[\"hamming_train\"].append(hamming)\n",
        "    #____________________________________________________________________________________________________________________\n",
        "    #Training loss val è loss per image,\n",
        "    #train_iter è count, numero di steps, images in training / batch size * epoch number\n",
        "    #train_acc over training\n",
        "    train_loss_val, train_iter, train_acc = loss_val/len(train_loader.dataset), count, correct/float(total)\n",
        "    \n",
        "    print(\"Training loss: \", train_loss_val, \" train acc: \",train_acc)    \n",
        "    ## Test Phase\n",
        "    \n",
        "    #Model switches to test phase\n",
        "    model.eval()\n",
        "    \n",
        "    all_outputs = []\n",
        "    all_targets = []\n",
        "    #Running through all mini batches in the dataset\n",
        "    count, correct, total, lost_val = test_iter, 0, 0, 0\n",
        "    for img_data, target in tqdm(val_loader, desc='Validation'):\n",
        "        img_data, target = img_data.to(device), target.to(device)\n",
        "        print(img_data.size())\n",
        "        output = model(img_data)\n",
        "        loss = criterion(output, target) #Cross entropy loss\n",
        "        c_loss = loss.data.item()\n",
        "        ax3.plot(count, c_loss, 'b.')\n",
        "        loss_val += c_loss\n",
        "        #Compute accuracy\n",
        "        #predicted = output.data.max(1)[1] #get index of max\n",
        "        total_batch = (target.size(0) * target.size(1))\n",
        "        total += total_batch\n",
        "        output_data = torch.sigmoid(output)>=0.5\n",
        "        target_data = (target==1.0)\n",
        "        #print(\"Predictions: \", output_data)\n",
        "        #print(\"Actual: \", target_data)\n",
        "        for arr1,arr2 in zip(output_data, target_data):\n",
        "            all_outputs.append(list(arr1.cpu().numpy()))\n",
        "            all_targets.append(list(arr2.cpu().numpy()))\n",
        "        c_acc = torch.sum((output_data == target_data.to(device)).to(torch.float)).item()\n",
        "        ax4.plot(count, c_acc/total_batch, 'b.')\n",
        "        correct += c_acc\n",
        "        count += 1\n",
        "    \n",
        "    #print(\"Outputs: \", len(all_outputs), \" x \", len(all_outputs[0]))\n",
        "    #print(\"Targets: \", len(all_targets), \" x \", len(all_targets[0]))\n",
        "    \n",
        "    #F1 Score\n",
        "    all_outputs = np.array(all_outputs)\n",
        "    all_targets = np.array(all_targets)\n",
        "    f1score_samples = f1_score(y_true=all_targets, y_pred=all_outputs, average='samples')\n",
        "    f1score_macro = f1_score(y_true=all_targets, y_pred=all_outputs, average='macro')\n",
        "    f1score_weighted = f1_score(y_true=all_targets, y_pred=all_outputs, average='weighted')\n",
        "    recall = recall_score(y_true=all_targets, y_pred=all_outputs, average='samples')\n",
        "    prec = precision_score(y_true=all_targets, y_pred=all_outputs, average='samples')\n",
        "    hamming = hamming_score(y_true=all_targets, y_pred=all_outputs)\n",
        "    \n",
        "    f1_scores[\"samples_test\"].append(f1score_samples)\n",
        "    f1_scores[\"macro_test\"].append(f1score_macro)\n",
        "    f1_scores[\"weighted_test\"].append(f1score_weighted)\n",
        "    f1_scores[\"hamming_test\"].append(hamming)\n",
        "    \n",
        "    #Accuracy over entire dataset\n",
        "    test_acc, test_iter, test_loss_val = correct/float(total), count, loss_val/len(test_loader.dataset)\n",
        "    print(\"Test set accuracy: \",test_acc)\n",
        "    print(\"f1_scores\", f1_scores)\n",
        "    train_results['epoch'].append(i)\n",
        "    train_results['train_loss'].append(train_loss_val)\n",
        "    train_results['train_acc'].append(train_acc)\n",
        "    train_results['train_iter'].append(train_iter)\n",
        "    \n",
        "    train_results['test_loss'].append(test_loss_val)\n",
        "    train_results['test_acc'].append(test_acc)\n",
        "    train_results['test_iter'].append(test_iter)\n",
        "    \n",
        "    #Save model with best accuracy\n",
        "    if test_acc > best_acc:\n",
        "        best_acc = test_acc\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "fig.savefig('train_curves.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4eBCcS-wmKI"
      },
      "source": [
        "#***Save***\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0_SQkFKsm57"
      },
      "source": [
        "model_name = \"test_recipe1m_13400_20_epochs\"\r\n",
        "\r\n",
        "\r\n",
        "PATH = \"/content/drive/My Drive/RecipeNet/model/\"+ model_name + \".pt\"\r\n",
        "\r\n",
        "torch.save({\r\n",
        "            'epoch': params[\"epochs\"],\r\n",
        "            'model_state_dict': model.state_dict(),\r\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\r\n",
        "            'f1_scores': f1_scores,\r\n",
        "            'train_results': train_results \r\n",
        "            }, PATH)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oF8XFzrUwwXz"
      },
      "source": [
        "#***Load***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JSOlq7LuA0O",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "a7d38ec0-b09d-4224-9a4d-2900c7154098"
      },
      "source": [
        "model_name = \"test_recipe1m_13400_images_10_epochs\"\r\n",
        "model_name = \"resnet_model_half_dataset\"\r\n",
        "\r\n",
        "#Definisco i plots\r\n",
        "train_results = defaultdict(list)\r\n",
        "train_iter, test_iter, best_acc = 0,0,0\r\n",
        "\r\n",
        "#dizionario f1 scores\r\n",
        "f1_scores = defaultdict(list)\r\n",
        "\r\n",
        "PATH = \"/content/drive/My Drive/RecipeNet/model/\"+model_name+\".pt\"\r\n",
        "checkpoint = torch.load(PATH)\r\n",
        "\r\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\r\n",
        "\r\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\r\n",
        "\r\n",
        "epoch = checkpoint['epoch']\r\n",
        "\r\n",
        "f1_scores = checkpoint['f1_scores']\r\n",
        "\r\n",
        "train_results = checkpoint['train_results']\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-88d09b1f4cb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'optimizer_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m-> 1052\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m   1053\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ResNet:\n\tsize mismatch for fc.weight: copying a param with shape torch.Size([227, 2048]) from checkpoint, the shape in current model is torch.Size([228, 2048]).\n\tsize mismatch for fc.bias: copying a param with shape torch.Size([227]) from checkpoint, the shape in current model is torch.Size([228])."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HgLjKiPkMLn"
      },
      "source": [
        "#***Evaluation***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmFAVmC8kRFp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55be6ee9-ad32-4498-a737-ff3a89f36324"
      },
      "source": [
        "#Model switches to test phase\r\n",
        "model.eval()\r\n",
        "\"\"\"\r\n",
        "all_outputs = []\r\n",
        "all_targets = []\r\n",
        "#Running through all mini batches in the dataset\r\n",
        "count, correct, total, lost_val = test_iter, 0, 0, 0\r\n",
        "for img_data, target in tqdm(test_loader, desc='Testing'):\r\n",
        "    img_data, target = img_data.to(device), target.to(device)\r\n",
        "    output = model(img_data)\r\n",
        "    loss = criterion(output, target) #Cross entropy loss\r\n",
        "    c_loss = loss.data.item()\r\n",
        "    ax3.plot(count, c_loss, 'b.')\r\n",
        "    loss_val += c_loss\r\n",
        "    #Compute accuracy\r\n",
        "    #predicted = output.data.max(1)[1] #get index of max\r\n",
        "    total_batch = (target.size(0) * target.size(1))\r\n",
        "    total += total_batch\r\n",
        "    output_data = torch.sigmoid(output)>=0.5\r\n",
        "    target_data = (target==1.0)\r\n",
        "    #print(\"Predictions: \", output_data)\r\n",
        "    #print(\"Actual: \", target_data)\r\n",
        "    for arr1,arr2 in zip(output_data, target_data):\r\n",
        "        all_outputs.append(list(arr1.cpu().numpy()))\r\n",
        "        all_targets.append(list(arr2.cpu().numpy()))\r\n",
        "    c_acc = torch.sum((output_data == target_data.to(device)).to(torch.float)).item()\r\n",
        "    correct += c_acc\r\n",
        "    count += 1\r\n",
        "\"\"\"\r\n",
        "#F1 Score\r\n",
        "all_outputs = np.array(all_outputs)\r\n",
        "all_targets = np.array(all_targets)\r\n",
        "f1score_samples = f1_score(y_true=all_targets, y_pred=all_outputs, average='samples')\r\n",
        "f1score_macro = f1_score(y_true=all_targets, y_pred=all_outputs, average='macro')\r\n",
        "f1score_weighted = f1_score(y_true=all_targets, y_pred=all_outputs, average='weighted')\r\n",
        "recall = recall_score(y_true=all_targets, y_pred=all_outputs, average='samples')\r\n",
        "prec = precision_score(y_true=all_targets, y_pred=all_outputs, average='samples')\r\n",
        "\r\n",
        "hamming = hamming_score(y_true=all_targets, y_pred=all_outputs)\r\n",
        "\r\n",
        "params[\"epochs\"] = 19\r\n",
        "print(\"__________________TRAIN RESULTS______________________\")\r\n",
        "print(\"F1_samples\", f1_scores[\"samples_train\"][params[\"epochs\"]])\r\n",
        "print(\"F1_macro\", f1_scores[\"macro_train\"][params[\"epochs\"]])\r\n",
        "print(\"F1_weighted\", f1_scores[\"weighted_train\"][params[\"epochs\"]])\r\n",
        "print(\"Hamming Score\",f1_scores[\"hamming_train\"][params[\"epochs\"]])\r\n",
        "\r\n",
        "print(\"__________________VAL RESULTS______________________\")\r\n",
        "print(\"F1_samples\", f1_scores[\"samples_test\"][params[\"epochs\"]])\r\n",
        "print(\"F1_macro\", f1_scores[\"macro_test\"][params[\"epochs\"]])\r\n",
        "print(\"F1_weighted\", f1_scores[\"weighted_test\"][params[\"epochs\"]])\r\n",
        "print(\"Hamming Score\",f1_scores[\"hamming_test\"][params[\"epochs\"]])\r\n",
        "\r\n",
        "\r\n",
        "print(\"__________________TEST RESULTS______________________\")\r\n",
        "print(\"F1_samples\", f1score_samples)\r\n",
        "print(\"F1_macro\", f1score_macro)\r\n",
        "print(\"F1_weighted\", f1score_weighted)\r\n",
        "print(\"Recall\",recall)\r\n",
        "print(\"Precision\",prec)\r\n",
        "print(\"Hamming Score\",hamming)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________TRAIN RESULTS______________________\n",
            "F1_samples 1.0\n",
            "F1_macro 0.09170305676855896\n",
            "F1_weighted 1.0\n",
            "Hamming Score 1.0\n",
            "__________________VAL RESULTS______________________\n",
            "F1_samples 0.32310558377735993\n",
            "F1_macro 0.03494751265912378\n",
            "F1_weighted 0.2827338585871737\n",
            "Hamming Score 0.22667604289701812\n",
            "__________________TEST RESULTS______________________\n",
            "F1_samples 0.32310558377735993\n",
            "F1_macro 0.03494751265912378\n",
            "F1_weighted 0.2827338585871737\n",
            "Recall 0.3841089466089466\n",
            "Precision 0.2983585858585858\n",
            "Hamming Score 0.22667604289701812\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1515: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  average, \"true nor predicted\", 'F-score is', len(true_sum)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Gvbw-eX6jxw"
      },
      "source": [
        "image_path = \"/content/drive/My Drive/RecipeNet/pizza.jpg\"\r\n",
        "from google.colab.patches import cv2_imshow\r\n",
        "#read as rgb image, resize and convert to range 0 to 1\r\n",
        "image = cv2.imread(image_path, 1)\r\n",
        "image = cv2.resize(image, params[\"img_size\"])/255\r\n",
        "\r\n",
        "print(len(new_ingredients), len(food_dict), len(col_names))\r\n",
        "image = (torch.from_numpy(image.transpose(2,0,1))).float() \r\n",
        "image = image.unsqueeze(0)\r\n",
        "image = image.to(device)\r\n",
        "\r\n",
        "output = model(image) \r\n",
        "\r\n",
        "output_data = torch.sigmoid(output)>=0.5\r\n",
        "\"\"\"\r\n",
        "print(output_data)\r\n",
        "print(\"output\", len(output_data.cpu().numpy()[0]))\r\n",
        "print(\"ingredients\",len(col_names))\r\n",
        "print(col_names)\"\"\"\r\n",
        "\r\n",
        "for i, elem in enumerate(output_data.cpu().numpy()[0]):\r\n",
        "    if elem:\r\n",
        "        print(col_names[i])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}