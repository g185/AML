%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% fphw Assignment
% LaTeX Template
% Version 1.0 (27/04/2019)
%
% This template originates from:
% https://www.LaTeXTemplates.com
%
% Authors:
% Class by Felipe Portales-Oliva (f.portales.oliva@gmail.com) with template 
% content and modifications by Vel (vel@LaTeXTemplates.com)
%
% Template (this file) License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[
	12pt, % Default font size, values between 10pt-12pt are allowed
	%letterpaper, % Uncomment for US letter paper size
	%spanish, % Uncomment for Spanish
]{fphw}

% Template-specific packages
\usepackage[utf8]{inputenc} % Required for inputting international characters
\usepackage[T1]{fontenc} % Output font encoding for international characters
\usepackage{multicol, latexsym, amsmath, amssymb}
\usepackage{blindtext}
\usepackage{subcaption}
\usepackage{caption}
\usepackage{wrapfig}
\usepackage{tabu}
\usepackage[dvipsnames]{xcolor}
\usepackage{floatflt}

\usepackage{graphicx} % Required for including images

\usepackage{booktabs} % Required for better horizontal rules in tables

\usepackage{listings} % Required for insertion of code

\usepackage{enumerate} % To modify the enumerate environment


%----------------------------------------------------------------------------------------
%	ASSIGNMENT INFORMATION
%----------------------------------------------------------------------------------------

\title{Assignment 2} % Assignment title

\author{Giuliano Martinelli 1915652, Gabriele Giannotta 1909375, Mario Dhimitri 1910181 } % Student name

\date{November 19th, 2020} % Due date

\institute{Sapienza Universit√† di Roma \\ Data Science} % Institute 

\class{Advanced Machine Learning} % Course or class name

\professor{Fabio Galasso} % Professor 

%----------------------------------------------------------------------------------------

\begin{document}

\maketitle % Output the assignment title, created automatically using the information in the custom commands above

%----------------------------------------------------------------------------------------
%	ASSIGNMENT CONTENT
%----------------------------------------------------------------------------------------

\section*{Question 2 - Backpropagation}
\section* {2. a}

Verify that the loss function defined in Eq. (1) has gradient w.r.t. $z^{(3)}$ as Eq. (2):

\begin{equation}
\begin{gathered}
$$
J\left(\theta,\left\{x_{i}, y_{i}\right\}_{i=1}^{N}\right)=\frac{1}{N} \sum_{i=1}^{N}-\log \left[\frac{\exp \left(z_{i}^{(3)}\right)_{y_{i}}}{\sum_{j=1}^{K} \exp \left(z_{i}^{(3)}\right)_{j}}\right]
$$
\end{gathered}
\end{equation}
\\
\begin{equation}
\begin{gathered}
$$
\frac{\partial J}{\partial z_{i}^{(3)}}\left(\theta,\left\{x_{i}, y_{i}\right\}_{i=1}^{N}\right)=\frac{1}{N}\left(\psi\left(z_{i}^{(3)}\right)-\delta_{i y_{i}}\right)
$$
\end{gathered}
\end{equation}
\\
Where $\delta$ is the Kronecker delta:

$$
\delta_{i j}=\left\{\begin{array}{ll}
1, & \text { if } i=j \\
0, & \text { otherwise }
\end{array}\right.
$$
\\
It is possible to verify the initial assumption by calculating the gradient:

\begin{enumerate}
\item $f(x) = \frac{1}{N} log(x) \rightarrow \frac{\partial f(x)}{\partial x} = -\frac{1}{Nx}$  \\ 

$J\left(\theta,\left\{x_{i}, y_{i}\right\}_{i=1}^{N}\right) = f(x) = - \frac {1}{N} log(\psi (z_{i}^{(3)})_{y_{i}}),\ \ \ \ \ \  x = \psi(z_{i}^{(3)})_{y_{i}} $ \\ 

$\frac{\partial J}{\partial \psi(z_{i}^{(3)})_{y_{i}}} = - \frac{1}{N \psi(z_{i}^{(3)})_{y_{i}}} = \frac{\partial J}{\partial a_{i}^{(3)}}$ \\


\item $f(x) = \psi(x) \rightarrow \frac{\partial f(x)}{\partial x} = \psi (x) (1 - \psi(x))$ \\

$ a_{i}^{(3)} = f(x) = \psi(z_{i}^{(3)}),\ \ \ \ \ \ x = z_{y_{i}}^{(3)}$ \\

$ \frac{\partial a_{i}^{(3)}}{\partial z_{i}^{(3)}} = \psi(z_{i}^{(3)})_{y_{i}} (1 - \psi(z_{i}^{(3)})_{y_{i}}) $ \\

\item $\frac{\partial J}{\partial z_{i}^{(3)}} = \frac{\partial J}{\partial a_{i}^{(3)}} \frac{\partial a_{i}^{(3)}}{\partial z_{i}^{(3)}} = - \frac{1}{N \psi(z_{i}^{(3)})_{y_{i}}} \ \psi(z_{i}^{(3)})_{y_{i}} (1 - \psi(z_{i}^{(3)})_{y_{i}}) = \frac{1}{N} (\psi(z_{i}^{(3)})_{y_{i}} - 1)$ \\

This because $\frac{\partial J}{\partial a_{i}^{(3)}}$ is the upstream gradient and $\frac{\partial a_{i}^{(3)}}{\partial z_{i}^{(3)}}$ is the local gradient.

\end{enumerate}

\newpage
\section* {2. b}



To verify that the partial derivative of the loss w.r.t. $W^{(2)}$ is:

$$
\begin{aligned}
\frac{\partial J}{\partial W^{(2)}}\left(\theta,\left\{x_{i}, y_{i}\right\}_{i=1}^{N}\right)=& \sum_{i=1}^{N} \frac{\partial J}{\partial z_{i}^{(3)}} \cdot \frac{\partial z_{i}^{(3)}}{\partial W^{(2)}} \\
&=\sum_{i=1}^{N} \frac{1}{N}\left(\psi\left(z_{i}^{(3)}\right)-\delta_{i y_{i}}\right) a_{i}^{(2)^{T}}
\end{aligned}
$$

We can use the property as follows:\\ 


$f(x) = aW, \ \ \ \ \ \ \frac{\partial f(x)}{\partial a} = W,\ \ \ \ \ \ \frac{\partial f(x)}{\partial W} = a$\\

Using upstream and local gradient, we can apply the chain rule:\\

$\frac{\partial J}{\partial W_2} = \frac{\partial J}{\partial z_{i}^{(3)}} \frac{\partial z_{i}^{(3)}}{\partial W_2} \ \ \ \ \ \ \ \ \ \frac{\partial z_{i}^{(3)}}{\partial W_2} = a_{i}^{(2)} \ \ \ \ \ \ since \ \  z_{i}^{(3)} = W_2 a_{i}^{(2)} + b $\\ \\

$\frac{\partial J}{\partial W_2} = \frac{1}{N} (\psi z_{i}^{(3)} - 1) a_{i}^{(2)}$\\

To verify that the regularized loss in Eq. (3) has the derivative as Eq. (4):


\begin{equation}
\begin{gathered}
$$
\tilde{J}\left(\theta,\left\{x_{i}, y_{i}\right\}_{i=1}^{N}\right)=\frac{1}{N} \sum_{i=1}^{N}-\log \left[\frac{\exp \left(z_{i}^{(3)}\right)_{y_{i}}}{\sum_{j=1}^{K} \exp \left(z_{i}^{(3)}\right)_{j}}\right]+\lambda\left(\left\|W^{(1)}\right\|_{2}^{2}+\left\|W^{(2)}\right\|_{2}^{2}\right)
$$
\end{gathered}
\end{equation}
\\
\begin{equation}
\begin{gathered}
$$
\frac{\partial \tilde{J}}{\partial W^{(2)}}=\sum_{i=1}^{N} \frac{1}{N}\left(\psi\left(z_{i}^{(3)}\right)-\delta_{i y_{i}}\right) a_{i}^{(2)^{T}}+2 \lambda W^{(2)}
$$
\end{gathered}
\end{equation}

We can do the following:\\

$f(x) = \lambda\left(\left\|W^{(1)}\right\|_{2}^{2}+\left\|W^{(2)}\right\|_{2}^{2}\right) = \lambda\left(\left\|W^{(1)}\right\|_{2}^{2}\right) + \lambda\left(\left\|W^{(2)}\right\|_{2}^{2}\right) = $\\ \\

$\frac{f}{W_2} = 0 + \lambda \frac{\partial(\sum\sum(W_2)^2)}{\partial W_2} = 2\lambda W_2$\\ \\

$\frac{\partial J}{\partial W_2} = \frac{1}{N} (\psi(z_{i}^{(3)}) - 1) a_{2}^{T} + 2\lambda W_2$


\newpage
\section* {2. c}

We now derive the expressions for the derivatives of the regularized loss in Eq. (3) w.r.t. W(1), b(1), b(2),\\
recalling that we used $\bigtriangleup_i$ to refer to the Kronecker delta. \\

\begin{itemize}

\item $\frac{\partial J}{\partial z_{i}^{(3)}} = - \frac{1}{N} (\psi (z_{i}^{(3)}) - \bigtriangleup_i$ \\ \\

\item $z_{i}^{(3)} = a_{i}^{(3)} W^{(2)} + b^{(2)}$, so as we have seen in 2.b: \\ 

$\frac {\partial J}{\partial W_2} = \frac {1}{N} (\psi (z_{i}^{(3)} - \bigtriangleup_i) a_{2i} $\\ \\

For the same reason:

\item $\frac {\partial J}{\partial a_{i}^{(2)}} = \frac{1}{N} (\psi (z_{i}^{(3)} - \bigtriangleup_i) W_2$ \\	\\

\item $\frac {\partial J}{\partial b_2} = \frac {\partial J}{\partial z_{i}^{(3)}} = \frac{1}{N} (\psi(z_{i}^{(3)}) - \bigtriangleup_i)$ \\ \\

Using the chain rule:

\item $\frac {\partial J}{\partial z_{i}^{(2)}} = \frac {\partial J}{\partial a_{i}^{(2)}} \frac {\partial a_{i}^{(2)}}{\partial z_{i}^{(2)}} \rightarrow a_{i}^{(2)}\left\{\begin{array}{ll}
0, & \text { if } z_{i}^{(2)}<0 \\
z_{i}^{(2)}, & \text { if } z_{i}^{(2)}\geq0
\end{array}\right.
\rightarrow \frac {\partial a_{i}^{(2)}}{\partial z_{i}^{(2)}} = \delta_i = 
\left\{\begin{array}{ll}
0, & \text { if } z_{i}^{(2)}<0 \\
1, & \text { if } z_{i}^{(2)}\geq0
\end{array}\right.
$ \\

$\frac {\partial J}{\partial z_{i}^{(2)}} = \frac{1}{N} (\psi(z_{i}^{(2)}) - \bigtriangleup_i) \delta_i$ \\ \\

\item $\frac {\partial J}{\partial b_1} = \frac {\partial J}{\partial z_{i}^{(2)}} = \frac{1}{N} (\psi(z_{i}^{(2)}) - \bigtriangleup_i) \delta_i$ \\ \\

\item $\frac {\partial J}{\partial W_1} = \frac {\partial J}{\partial z_{i}^{(2)}} a_{1}^{(1)} = \frac{1}{N} (\psi(z_{i}^{(2)}) - \bigtriangleup_i) \delta_i \ a_{1}^{(1)}$ \\ \\

\item $\frac {\partial J}{\partial a_i} = \frac {\partial J}{\partial z_{i}^{(2)}} W_1 = \frac{1}{N} (\psi(z_{i}^{(2)}) - \bigtriangleup_i) \delta_i \ W_1$

\end{itemize}

\newpage

\section*{Question 4 - Multi-layer perceptron using PyTorch}
\section* {4. c}

\begin{table}[ht]
\begin{center}
\caption{Train and Test Accuracy for multiple layers.}
\begin{tabular}{rrlll}
\toprule
 Total Layers &  Epochs & Hidden Layer Size & Train Accuracy & Test Accuracy \\
\midrule
            3 &      20 &            250,\ 50 &         55.3 \% &        56.8 \% \\
            2 &      20 &               100 &         55.8 \% &        54.0 \% \\
            3 &      10 &            250,\ 50 &         53.4 \% &        53.1 \% \\
            2 &      10 &               100 &         52.2 \% &        51.5 \% \\
            5 &      20 &   200,\ 200,\ 250,\ 150 &          9.8 \% &        10.0 \% \\
            4 &      10 &       150,\ 100,\ 200 &          9.8 \% &         9.0 \% \\
            4 &      20 &       150,\ 100,\ 200 &          9.8 \% &         9.0 \% \\
            5 &      10 &   200,\ 200,\ 250,150 &          9.8 \% &         9.0 \% \\
\bottomrule
\end{tabular}
\end{center}
\end{table}











%------------------------------------------------
\end{document}
